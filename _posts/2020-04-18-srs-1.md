---
layout: post
title:  "Book summary – Building Secure and Reliable Systems: Chapter 1"
date:   2020-04-18 12:00:00 +0300
tags:   google-srs book-summary
---

This series is a terse summary of the book
[*Building Secure and Reliable Systems*][sre]
by Heather Adkins, Betsy Beyer, Paul Blankinship, Ana Oprea,
Piotr Lewandowski, Adam Stubblefield.
Experts from Google share best practices to help design scalable and reliable systems that are fundamentally secure.

[sre]: https://landing.google.com/sre/books/

### Chapter 1: Intersection of Security and Reliability

Reliability issues usually have _nonmalicious_ origin
while security issues are typically caused by _adversaries_.
That is, reliability incidents are usually unintentional with indirect failures
while security issues are caused by an adversary who might _actively_ lead the system to fail.
**Fail safe** (open) vs. **fail secure** (closed) principle is a handy way
to prioritize issues you defend from.

- [ ] System failure mode: fail safe or fail secure?

Reliability incidents are better managed by a team of _multiple responders_
who have diverse perspectives that might come into help.
Security incidents are better managed by _the smallest_ possible number of competent people,
in order to avoid alerting the adversary.

- [ ] Who is your response team and why?

**Confidentiality, Integrity, Availablity** trio (aka “CIA”).
Both secure and reliable systems desire these properties
but for different reasons with different intentions.
They might even come into conflict because of that.

Both reliability and security are _hard to bolt onto_ existing system
so it’s better to take them into account from the early design stages
and maintain throughout the software lifecycle.
These considerations are typically _invisible_ until a (catastrophic) failure occurs
so reliability and security are often dismissed, deprioritized, or debudgeted
without proper **risk assessment**.

Designing _simple_ systems is a decent way to provide some degree of reliability and security
in an “accidental” manner, without dedicated design effort specifically for that.
However, _evolution_ might turn an initially simple system into a mess.
Minor and seemingly innocuous changes might lead to a death by a thousand cuts or cause a butterfly effect.

**Defence in depth** provides independent interlocks
which can be kept individually simple, with limited “blast radius” in case of failure.
_Principle of least privilege_ has a similar intent
along with _multiparty authorization_ approaches.

- [ ] ~~Do you need~~ _How much_ security and reliability do you need?
- [ ] Answer this question during early design stage
- [ ] Perform risk assessment for reliability and security

While design stage is definitely important, you should not limit your efforts to it alone.
Code review and code reuse might **prevent issues** before QA stage.
Testing and fuzzing might catch issues before deployment.
Slow rollouts and canaries might catch issues before they hit users.
That is, “defence in depth” applies to software lifecycle too.

- [ ] Be aware of _Secure Software Development Lifecycle_ (SSDLC)

Perfect security and reliability are impossible in practice.
Preventive measures _will fail_, design with this in mind and have a **recovery plan**.
Crisis response is usually hectic and tense, don’t turn it into a clueless panic.
During a disaster it is essential to keep a cool head,
maintain a clear chain of command,
and have prepared a solid set of checklists, playbooks, protocols.
These need to be prepared _beforehand_, not during the incident.
Preventive measures tend to rot over time when stored not in use
so _drills and exercises_ should be used to keep them sharp.
Patching a system during recovery might turn into a _compromise_
between immediate risks caused by the failing system
and future risks introduced by quickly applied changes.
Have a plan for after the incident too.

- [ ] Have a disaster recovery plan
- [ ] Perform drills and exercises
